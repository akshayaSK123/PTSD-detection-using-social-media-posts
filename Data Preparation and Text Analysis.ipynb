{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "!pip install wordcloud\n",
    "!pip install emot\n",
    "!pip install emoji\n",
    "!pip install textblob  \n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "!pip install pyLDAvis\n",
    "import pyLDAvis\n",
    "from pyLDAvis import sklearn as sklearn_lda\n",
    "import pickle \n",
    "\n",
    "!pip install corextopic\n",
    "from corextopic import corextopic as ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"\"#path containing the files\n",
    "all_files = os.listdir(path)\n",
    "print(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading text files of depressed individuals\n",
    "path = \"\"#path contained depressed people's posts\n",
    "all_files = os.listdir(path)\n",
    "\n",
    "textlist = []\n",
    "for user in all_files:\n",
    "    userpath = path+ str(user)\n",
    "    files = os.listdir(userpath)\n",
    "    txt_files = list(filter(lambda x: x[-4:] == '.txt', files))\n",
    "\n",
    "#Going through all the user posts and appending them to textlist\n",
    "    for i in range(len(txt_files)):\n",
    "        post = []\n",
    "        text = open(path+str(user)+\"/\"+ txt_files[i],\"r\" ,encoding=\"utf8\")\n",
    "        for line in text:\n",
    "            post.append(line.strip().lower())\n",
    "        concat = \"\".join(post)\n",
    "        textlist.append((user,concat,1))\n",
    "        text.close()\n",
    "\n",
    "textlist[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading text files of non-depressed individuals\n",
    "path = \"\"#Path containing text files of non-depressed individuals\n",
    "all_files = os.listdir(path)\n",
    "\n",
    "non_textlist = []\n",
    "for user in all_files:\n",
    "    userpath = path+ str(user)\n",
    "    files = os.listdir(userpath)\n",
    "    txt_files = list(filter(lambda x: x[-4:] == '.txt', files))\n",
    "\n",
    "#Going through all the user posts and appending them to textlist    \n",
    "    for i in range(len(txt_files)):\n",
    "        post = []\n",
    "        text = open(path+str(user)+\"/\"+ txt_files[i],\"r\" ,encoding=\"utf8\")\n",
    "        for line in text:\n",
    "            post.append(line.strip().lower())\n",
    "        concat = \"\".join(post)\n",
    "        non_textlist.append((user,concat,0))\n",
    "        text.close()\n",
    "        \n",
    "non_textlist[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 50)\n",
    "totallist = textlist + non_textlist\n",
    "text = pd.DataFrame(totallist, columns=[\"UserId\", \"Post\",\"Depressed\"]) #pd.dataframe(np.array(textlist))\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = list(text['Post']) \n",
    "posts[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining functions to apply for the Instagram DataFrame with posts\n",
    "\n",
    "def find_mentioned(tweet):\n",
    "    #'''This function will extract the twitter handles of people mentioned in the tweet'''\n",
    "    return re.findall('(?<!RT\\s)(@[A-Za-z]+[A-Za-z0-9-_]+)', tweet)\n",
    "\n",
    "def find_hashtags(tweet):\n",
    "    #'''This function will extract hashtags'''\n",
    "    return re.findall('(#[A-Za-z]+[A-Za-z0-9-_]+)', tweet) \n",
    "\n",
    "import emoji\n",
    "def extract_emojis(str):\n",
    "  return ''.join(c for c in str if c in emoji.UNICODE_EMOJI)\n",
    "\n",
    "def count_hashtags(post):\n",
    "    return [Hashtags.count(w) for w in Hashtags]\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "my_stopwords = nltk.corpus.stopwords.words('english')\n",
    "my_stopwords.extend(['ve','th','re'])\n",
    "def remove_stop_words(tweet):\n",
    "    joined_tweet = []\n",
    "    tweet_token_list = [word.strip() for word in tweet.split(' ')\n",
    "                            if word not in my_stopwords]\n",
    "    #tweet_tokens_joined = tweet_token_list.join(' ')\n",
    "    return tweet_token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "def find_senti_pos(tweet):\n",
    "    #'''This function will extract postive sentiment'''\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    #message_text = '''Like you, I am getting very frustrated with this process. I am genuinely trying to be as reasonable as possible. I am not trying to \"hold up\" the deal at the last minute. I'm afraid that I am being asked to take a fairly large leap of faith after this company (I don't mean the two of you -- I mean Enron) has screwed me and the people who work for me.'''\n",
    "    scores = sid.polarity_scores(tweet)\n",
    "    for key in sorted(scores):\n",
    "        return(scores['pos'])\n",
    "\n",
    "def find_senti_neg(tweet):\n",
    "    #'''This function will extract negative sentiment'''\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    #message_text = '''Like you, I am getting very frustrated with this process. I am genuinely trying to be as reasonable as possible. I am not trying to \"hold up\" the deal at the last minute. I'm afraid that I am being asked to take a fairly large leap of faith after this company (I don't mean the two of you -- I mean Enron) has screwed me and the people who work for me.'''\n",
    "    scores = sid.polarity_scores(tweet)\n",
    "    for key in sorted(scores):\n",
    "        return(scores['neg'])\n",
    "\n",
    "def find_senti_neu(tweet):\n",
    "    #'''This function will extract neutral sentiment'''\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    #message_text = '''Like you, I am getting very frustrated with this process. I am genuinely trying to be as reasonable as possible. I am not trying to \"hold up\" the deal at the last minute. I'm afraid that I am being asked to take a fairly large leap of faith after this company (I don't mean the two of you -- I mean Enron) has screwed me and the people who work for me.'''\n",
    "    scores = sid.polarity_scores(tweet)\n",
    "    for key in sorted(scores):\n",
    "        return(scores['neu'])\n",
    "    \n",
    "def find_senti_overall(tweet):\n",
    "    #'''This function will extract overall sentiment'''\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    #message_text = '''Like you, I am getting very frustrated with this process. I am genuinely trying to be as reasonable as possible. I am not trying to \"hold up\" the deal at the last minute. I'm afraid that I am being asked to take a fairly large leap of faith after this company (I don't mean the two of you -- I mean Enron) has screwed me and the people who work for me.'''\n",
    "    scores = sid.polarity_scores(tweet)\n",
    "    for key in sorted(scores):\n",
    "        return(scores['compound'])    \n",
    "\n",
    "def find_senti(tweet):\n",
    "    #'''This function will extract type of sentiment'''\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    #message_text = '''Like you, I am getting very frustrated with this process. I am genuinely trying to be as reasonable as possible. I am not trying to \"hold up\" the deal at the last minute. I'm afraid that I am being asked to take a fairly large leap of faith after this company (I don't mean the two of you -- I mean Enron) has screwed me and the people who work for me.'''\n",
    "    scores = sid.polarity_scores(tweet)\n",
    "    for key in sorted(scores):\n",
    "        if scores['compound']  >= 0.05 :\n",
    "          result='positive'\n",
    "        elif scores['compound']  <= -0.05 :\n",
    "          result='negative'\n",
    "        else:\n",
    "          result='neutral'\n",
    "        return(result)  \n",
    "    \n",
    "def find_count_of_neg(tweet):\n",
    "    #'''This function will find the count of negative tweets'''\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    scores = sid.polarity_scores(tweet)\n",
    "    for key in sorted(scores):\n",
    "        if scores['compound']  <= -0.05 :\n",
    "          result=1\n",
    "        else:\n",
    "          result=0\n",
    "        return(result)\n",
    "    \n",
    "def find_count_of_posi(tweet):\n",
    "    #'''This function will find the count of positive tweets'''\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    scores = sid.polarity_scores(tweet)\n",
    "    for key in sorted(scores):\n",
    "        if scores['compound']  >= 0.05 :\n",
    "          result=1\n",
    "        else:\n",
    "          result=0\n",
    "        return(result)\n",
    "    \n",
    "def find_count_of_neu(tweet):\n",
    "    #'''This function will find the count of neutral tweets'''\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    scores = sid.polarity_scores(tweet)\n",
    "    for key in sorted(scores):\n",
    "        if scores['compound']  >= 0.05 :\n",
    "          result=0\n",
    "        elif scores['compound']  <= -0.05 :\n",
    "          result=0\n",
    "        else:\n",
    "          result=1\n",
    "        return(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "a = map(len, map(nltk.word_tokenize, posts))\n",
    "\n",
    "text['number_of_words'] = list(a)\n",
    "text['Hashtags'] = text['Post'].apply(find_hashtags)  #Finding Hashtags\n",
    "text['Post_processed'] = text['Post'].map(lambda x: re.sub('[0-9•<#,%():\".!?-]', ' ', x))  #Remove punctuation\n",
    "text['Post_processed'] = text['Post_processed'].map(lambda x: re.sub('@\\w+', ' ', x))  #Remove user mentions\n",
    "text['number_of_hashtags'] = [len(x) for x in text['Hashtags']]  #Count Hashtags\n",
    "text['sentimentpositive'] = text['Post_processed'].apply(find_senti_pos)\n",
    "text['sentimentnegative'] = text['Post_processed'].apply(find_senti_neg)\n",
    "text['sentimentneutral'] = text['Post_processed'].apply(find_senti_neu)\n",
    "text['sentimentcompound'] = text['Post_processed'].apply(find_senti_overall)\n",
    "text['sentimentoverall'] = text['Post_processed'].apply(find_senti)\n",
    "text['polarity'] = text['Post_processed'].map(lambda text: TextBlob(text).sentiment.polarity) \n",
    "text['negativepost'] = text['Post_processed'].apply(find_count_of_neg)\n",
    "text['positivepost'] = text['Post_processed'].apply(find_count_of_posi)\n",
    "text['neutralpost'] = text['Post_processed'].apply(find_count_of_neu)\n",
    "text.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_words=text.groupby(['UserId']).mean()\n",
    "avg_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = text.groupby(['UserId']).sum()\n",
    "total_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_words = text.groupby(['UserId']).count()\n",
    "count_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##########################  Data Cleaning  #########################\n",
    "\n",
    "#Converting Instagram posts to user level of detail from post level of detail for both raw and processed text\n",
    "temp_posts = pd.DataFrame(text.groupby('UserId')['Post'].apply(lambda x: \"%s\" % ' '.join(x)))  \n",
    "a = pd.DataFrame(text.groupby('UserId')['Post_processed'].apply(lambda x: \"%s\" % ' '.join(x)))\n",
    "\n",
    "a['Post'] = temp_posts['Post']\n",
    "\n",
    "#Replacing anything except alphabets with blank (to remove emojis)\n",
    "a['Post_processed'] = a.Post_processed.str.replace('[^a-zA-Z\\']', ' ')\n",
    "\n",
    "#Getting Hashtag details at user level from post\n",
    "a['Hashtags'] = a['Post'].apply(find_hashtags)\n",
    "a['number_of_hashtags'] = [len(x) for x in a['Hashtags']]\n",
    "a['average_hashtags_per_post'] = avg_words['number_of_hashtags']\n",
    "\n",
    "#Getting Emojis from posts\n",
    "a['Emojis'] = a['Post'].apply(extract_emojis)\n",
    "\n",
    "#Removing stop words using NLTK english library\n",
    "a['Post_processed'] = a['Post_processed'].apply(remove_stop_words).apply(lambda x: ' '.join(x))\n",
    "\n",
    "#Appending aggregated values to user level DF\n",
    "a['number_of_words'] = total_words['number_of_words']\n",
    "a['average_words_per_post'] = avg_words['number_of_words']\n",
    "a['Depressed'] = avg_words['Depressed']\n",
    "a['avg_sentimentpositive'] = avg_words['sentimentpositive']\n",
    "a['avg_sentimentnegative'] = avg_words['sentimentnegative']\n",
    "a['avg_sentimentneutral'] = avg_words['sentimentneutral']\n",
    "a['avg_sentimentcompound'] = avg_words['sentimentcompound']\n",
    "a['cnt_neg_posts'] = total_words['negativepost']\n",
    "a['cnt_neu_posts'] = total_words['neutralpost']\n",
    "a['cnt_pos_posts'] = total_words['positivepost']\n",
    "a['avg_polarity'] = avg_words['polarity'] \n",
    "a.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting word features out of the processed text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "features = tfidf.fit_transform(a.Post_processed).toarray()\n",
    "labels = a.Depressed\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required packages and creating data splits for training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(a['Post_processed'], a['Depressed'], random_state = 0)\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "models = [\n",
    "    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0,solver='lbfgs'),\n",
    "]\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "\n",
    "for model in models:\n",
    "  model_name = model.__class__.__name__\n",
    "  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n",
    "  precisions = cross_val_score(model, features, labels, scoring='precision', cv=CV)  \n",
    "  recalls = cross_val_score(model, features, labels, scoring='recall', cv=CV)\n",
    "  appended = []\n",
    "  for i in range(len(accuracies)):\n",
    "    appended.append((accuracies[i],precisions[i],recalls[i]))\n",
    "    \n",
    "  for fold_idx, appended in enumerate(appended):\n",
    "    entries.append((model_name, fold_idx, appended[0],appended[1],appended[2]))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy','precision','recall'])\n",
    "print(cv_df)\n",
    "import seaborn as sns\n",
    "sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='accuracy', data=cv_df, \n",
    "              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df.groupby('model_name').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing Model\n",
    "\n",
    "**From the above values of evaluation metrics (accuracy, precision and recall) it is clear that _Linear SVC_ model suits better for analyzing the instagram posts. We build the model on the training data as the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building model using Linear SVC algorithm\n",
    "model = LinearSVC()\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, a.index, test_size=0.33, random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#Printing the cofusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Printing the data points that were misclassified by the model\n",
    "category_id_df = pd.DataFrame({'Name':['Depressed','Non-Depressed'],'Value':[1.0,0.0]})\n",
    "category_to_id = dict(category_id_df.values)\n",
    "id_to_category = dict(category_id_df[['Value', 'Name']].values)\n",
    "\n",
    "from IPython.display import display\n",
    "for predicted in a.Depressed.unique():\n",
    "  for actual in a.Depressed.unique():\n",
    "    if predicted != actual:\n",
    "      print(\"'{}' predicted as '{}' : {} examples.\".format(id_to_category[actual], id_to_category[predicted],conf_mat[int(actual), int(predicted)]))\n",
    "      df =  pd.DataFrame(a.loc[indices_test[(y_test == actual) & (y_pred == predicted)]])\n",
    "      if int(conf_mat[int(actual), int(predicted)]) >= 1 :\n",
    "          display(df[[ 'Depressed','Post']])\n",
    "      print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the misclassified posts into a file to analyze and understand them better\n",
    "print(df[['Post']])\n",
    "df.to_csv(r'D:\\study stuff\\UIC\\Sem - 2\\IDS 594 Healthcare\\Project\\Misclassified.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the evaluation metrics for Depressed and Non-Depressed classes for better analysis\n",
    "\n",
    "from sklearn import metrics\n",
    "print(metrics.classification_report(y_test, y_pred,target_names= ['Non-Depressed','Depressed'] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the DataFrame created to an excel for Exploratory Data Analysis in R\n",
    "\n",
    "a.reset_index(level=0, inplace=True)  #To convert UserID which is index into a column at the same position\n",
    "DataFrame = a[['UserId', 'number_of_hashtags','average_hashtags_per_post','number_of_words', 'average_words_per_post','Depressed']]\n",
    "DataFrame.to_csv(r'D:\\study stuff\\UIC\\Sem - 2\\IDS 594 Healthcare\\Project\\DataFrame.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prearing a long string with all the posts of Depressed individuals\n",
    "Depressed = a.loc[ a['Depressed'] == 1.0 ]\n",
    "long_string = ' '.join(Depressed['Post'])\n",
    "import emoji\n",
    "def extract_emojis(str):\n",
    "  return ''.join(c for c in str if c in emoji.UNICODE_EMOJI)\n",
    "emojis = extract_emojis(long_string)\n",
    "emojis[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the most used emojis by depressed individuals\n",
    "emojifreq = [emojis.count(w) for w in emojis]\n",
    "emojifrequency = list(set(zip(emojis, emojifreq)))\n",
    "emojifrequency.sort(key=lambda x: x[1],reverse=True)\n",
    "emojidf = pd.DataFrame(emojifrequency, columns=[\"Emoji\", \"Frequency\"])\n",
    "print(emojifrequency[0:5])\n",
    "emojidf.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load the library with the CountVectorizer method to get the most frequest words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "# Helper function\n",
    "def plot_10_most_common_words(count_data, count_vectorizer):\n",
    "    import matplotlib.pyplot as plt\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    total_counts = np.zeros(len(words))\n",
    "    for t in count_data:\n",
    "        total_counts+=t.toarray()[0]\n",
    "    \n",
    "    count_dict = (zip(words, total_counts))\n",
    "    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n",
    "    words = [w[0] for w in count_dict]\n",
    "    counts = [w[1] for w in count_dict]\n",
    "    x_pos = np.arange(len(words)) \n",
    "    \n",
    "    plt.figure(2, figsize=(15, 15/1.6180))\n",
    "    plt.subplot(title='10 most common words')\n",
    "    sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "    sns.barplot(x_pos, counts, palette='husl')\n",
    "    plt.xticks(x_pos, words, rotation=90) \n",
    "    plt.xlabel('words')\n",
    "    plt.ylabel('counts')\n",
    "    plt.show()\n",
    "# Initialise the count vectorizer with the English stop words\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "# Fit and transform the processed titles\n",
    "count_data = count_vectorizer.fit_transform(Depressed['Post_processed']) #Using only the posts of \"Depressed\" individuals\n",
    "# Visualise the 10 most common words\n",
    "plot_10_most_common_words(count_data, count_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (Unsupervised LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing LDA using sklearn\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "# Load the LDA model from sk-learn\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    " \n",
    "# Helper function\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        \n",
    "# Tweak the two parameters below\n",
    "number_topics = 4\n",
    "number_words = 30\n",
    "# Create and fit the LDA model\n",
    "lda = LDA(n_components=number_topics, n_jobs=-1)\n",
    "lda.fit(count_data)\n",
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, count_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the .html file for LDA to visualize the topics on first and second principal components (PC1 and PC2)\n",
    "\n",
    "LDAvis_data_filepath = os.path.join('./LDA_'+str(number_topics))\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "\n",
    "LDAvis_prepared = sklearn_lda.prepare(lda, count_data, count_vectorizer)\n",
    "with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "        \n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath,'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "pyLDAvis.save_html(LDAvis_prepared, './LDA_'+ str(number_topics) +'.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Explanation (CorEx algorithm WITHOUT anchor words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Explanation (CorEx) WITHOUT anchor words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=.5,\n",
    "    min_df=10,\n",
    "    max_features=None,\n",
    "    ngram_range=(1, 2),\n",
    "    norm=None,\n",
    "    binary=True,\n",
    "    use_idf=False,\n",
    "    sublinear_tf=False\n",
    ")\n",
    "vectorizer = vectorizer.fit(depressed['Post_processed'])\n",
    "tfidf = vectorizer.transform(depressed['Post_processed'])\n",
    "#vectorizer = vectorizer.fit(text['Post_processed'])\n",
    "#tfidf = vectorizer.transform(text['Post_processed'])\n",
    "pd.DataFrame(tfidf)\n",
    "\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print(len(vocab))\n",
    "\n",
    "anchors = []\n",
    "model = ct.Corex(n_hidden=5, seed=42)\n",
    "model = model.fit(tfidf, words=vocab)\n",
    "\n",
    "for i, topic_ngrams in enumerate(model.get_topics(n_words=30)):\n",
    "    topic_ngrams = [ngram[0] for ngram in topic_ngrams if ngram[1] > 0]\n",
    "    print(\"Topic #{}: {}\".format(i+1, \", \".join(topic_ngrams)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Explanation(CorEx algorithm WITH anchor words- Supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anchors designed to nudge the model towards measuring specific genres\n",
    "anchors = [\n",
    "     #Causes\n",
    "    [\"accident\", \"assault\",\"abuse\",\"trauma\",\"war\",\"loss\",\"torture\",\"lost\"],\n",
    "     #Coping Mechanisms\n",
    "    [\"anxiety\",\"ptsd\",\"depression\",\"insomnia\",\"sleep\",\"irritability\",\"irritated\",\"mental\",\"illness\"], \n",
    "     #Symptoms\n",
    "    [\"positive\",\"meditation\",\"love\",\"loved ones\",\"family\",\"kids\",\"suport\",\"art\",\"emotions\"], \n",
    "     #Treatment options\n",
    "    [\"therapy\",\"paroxetine\",\"sertraline\",\"amitriptyline\",\"phenelzine\",\"antidepressant\",\"mediciene\",\"depressant\"],\n",
    "     #Physical\n",
    "    [\"pain\",\"sweat\",\"sick\",\"tremble\",\"harm\",\"drugs\",\"alcohol\",\"dizzy\",\"ache\"]\n",
    "]\n",
    "anchors = [\n",
    "    [a for a in topic if a in vocab]\n",
    "    for topic in anchors\n",
    "]\n",
    "\n",
    "model = ct.Corex(n_hidden=5, seed=42)\n",
    "model = model.fit(\n",
    "    tfidf,\n",
    "    words=vocab,\n",
    "    anchors=anchors, # Pass the anchors in here\n",
    "    anchor_strength=3 # Tell the model how much it should rely on the anchors\n",
    ")\n",
    "\n",
    "for i, topic_ngrams in enumerate(model.get_topics(n_words=30)):\n",
    "    topic_ngrams = [ngram[0] for ngram in topic_ngrams if ngram[1] > 0]\n",
    "    print(\"Topic #{}: {}\".format(i+1, \", \".join(topic_ngrams)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
